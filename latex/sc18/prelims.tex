% !TEX root = paper.tex

\section{Preliminaries} 
\label{sec:prelims}

	Tensors will be denoted using Euler script (e.g., $\T{T}$), matrices will be denoted (e.g., $\M{M}$), vectors will be denoted as lower case bold face type (e.g., $\V{v}$), and scalars will not be bolded (e.g., $s$). Matlab style notation will be sued to index into tensors, matrices, and vectors. For example, $\M{M}(:,c)$ gives the cth column of the matrix M.
	The Hadamard product is the element wise matrix product and will be denoted using $*$. 
	The Khatri-Rao product, abbreviated KRP, will be denoted with $\Khat$. Given matrices $\M{A}$ and $\M{B}$ that are $I_{A} \times R$ and $I_{B} \times R$, the KRP $\M{K} = \M{A} \Khat \M{B}$ is $I_{A}I_{B} \times R$. It can be thought of as a series row-wise Hadamard products s.t. \\$\M{K}(0:) = \M{A}(0,:) * \M{B}(0,:)\\ \M{K}(1:) = \M{A}(0,:) * \M{B}(1,:)  \\.\\.\\. \\ \M{K}(I_{A}I_{B} - 1,:) = \M{A}(I_{A}-1,:) * \M{B}(I_{B}-1,:)$ 

\begin{itemize}
	%\item tensor notation, Hadamard, Khatri-Rao, 
	\item NNCP definition
	\item BCD/ALS
	\begin{itemize}
		\item MU, HALS, BPP, Nesterov, ADMM
	\end{itemize}
	\item MPI collectives
\end{itemize}

\begin{algorithm}
%\caption{$(\CPl,\epsilon) = \text{NNCP}(\TA,k)$}
\caption{$\CP = \text{NNCP}(\TA,k)$}
\label{alg:nncp}
\begin{algorithmic}[1]
\Require $\TA$ is $I_1\times \cdots \times I_N$ tensor, $k$ is approximation rank
\State \Comment{Initialize data}
%\State $\alpha = \|\TA\|^2$
\For{$n=2$ to $N$}
	\State Initialize $\Mn{H}{n}$ 
	\State $\Mn{G}{n} = \MnTra{H}{n}\Mn{H}{n}$
\EndFor
\State \Comment{Compute NNCP approximation}
\While{not converged}
	\State \Comment{Perform outer iteration of BCD}
	\For{$n=1$ to $N$}
	\State \Comment{Compute new factor matrix in $n$th mode}
	\State $\Mn{M}{n} = \text{MTTKRP}(\TA,\{\Mn{H}{i}\},n)$
	\State $\Mn{S}{n} = \Mn{G}{1} \Hada \cdots \Hada \Mn{G}{n-1} \Hada \Mn{G}{n+1} \Hada \cdots \Hada \Mn{G}{N}$
	\State $\Mn{H}{n} = \text{NLS-Update}(\Mn{S}{n},\Mn{M}{n})$
%	\State $\Mn[\hat]{H}{n} = \text{NLS-Update}(\Mn{S}{n},\Mn{M}{n})$
%	\State \Comment{Normalize columns}
%	\State $\V{\lambda} = \text{Col-Norms}(\Mn[\hat]{H}{n})$
%	\State $\Mn{H}{n} = \text{Col-Scale}(\Mn[\hat]{H}{n},\V{\lambda})$
%	\State \Comment{Organize data for later modes}
	\State $\Mn{G}{n} = \MnTra{H}{n}\Mn{H}{n}$
	\EndFor
%	\State \Comment{Compute relative error $\epsilon$ from mode-$N$ matrices}
%	\State $\beta = \langle \Mn{M}{N},\Mn[\hat]{H}{N} \rangle$
%	\State $\gamma = \V{\lambda}^\Tra (\Mn{S}{N} \Hada \Mn{G}{N}) \V{\lambda}$
%	\State $\epsilon = \sqrt{(\alpha-2\beta+\gamma)/\alpha}$ 
\EndWhile
%\Ensure $\|\TA - \dsquare{\V{\lambda}; \Mn{H}{1},\dots,\Mn{H}{N}}\| /\|\TA\| = \epsilon$
\Ensure $\TA \approx \CP$
\end{algorithmic}
\end{algorithm}

\subsection {Block Coordinate Descent(BCD) for NTF:}

In this section, we will see relevant foundation for using  this
framework.  The explanation of this section is mainly motivated out of
Kim, He and Park \cite{KHP2014}. Consider a constrained non-linear optimization problem
as follows:
\begin{gather}
\min f(x)\:\mbox{ subject to }\:x \in\mathcal{X},\label{eq:general_nonlinear}
\end{gather}
Here,  $\mathcal{X}$ is a closed convex subset of $\mathbb{R}^{n}$.
An important assumption to be exploited in the BCD method is that
the set $\mathcal{X}$ is represented by a Cartesian product:
\begin{equation}
\mathcal{X}=\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m},\label{eq:bcd-cartesian-product}
\end{equation}
where $\mathcal{X}_{j}$, $j=1,\cdots,m$, is a closed convex subset
of $\mathbb{R}^{N_{j}}$, satisfying $n=\sum_{j=1}^{m}N_{j}$.
Accordingly, the vector $\mathbf{x}$ is partitioned as
$\mathbf{x}=(\mathbf{x}_{1},\cdots,\mathbf{x}_{m})$ so that
$\mathbf{x}_{j}\in\mathcal{X}_{j}$ for $j=1,\cdots,m$. The BCD
method solves for $\mathbf{x}_{j}$ by fixing all other subvectors of
$\mathbf{x}$ in a cyclic manner. That is, if
$\mathbf{x}^{(i)}=(\mathbf{x}_{1}^{(i)},\cdots,\mathbf{x}_{m}^{(i)})$
is given as the current iterate at the $i^{th}$ step, the algorithm
generates the next iterate
$\mathbf{x}^{(i+1)}=(\mathbf{x}_{1}^{(i+1)},\cdots,\mathbf{x}_{m}^{(i+1)})$
block by block, according to the solution of the following
subproblem:
\begin{equation}
\mathbf{x}_{j}^{(k+1)}\leftarrow\underset{\mathbf{\xi}\in\mathcal{X}_{j}}{\text{argmin}}
f(\mathbf{x}_{1}^{(k+1)},\cdots,\mathbf{x}_{j-1}^{(k+1)},\mathbf{\xi},\mathbf{x}_{j+1}^{(k)},\cdots,\mathbf{x}_{m}^{(k)}).\label{eq:bcd-method}
\end{equation}
Also known as a \textit{non-linear Gauss-Seidel}
method~\cite{Bertsekas1999}, this algorithm updates one block each
time, always using the most recently updated values of other blocks
$\mathbf{x}_{\tilde{j}},\tilde{j}\ne j$. This is important since it
ensures that after each update, the objective function value does
not increase. For a sequence $\left\lbrace
\mathbf{x}^{(i)}\right\rbrace $ where each $\mathbf{x}^{(i)}$ is
generated by the BCD method, the following property holds.
\begin{theorem}
\label{thm:bcd}
Suppose $f$ is continuously differentiable in $\mathcal{X}=\mathcal{X}_{1}\times\dots\times\mathcal{X}_{m}$,
where $\mathcal{X}_{j}$, $j=1,\cdots,m$, are closed convex sets.
Furthermore, suppose that for all $j$ and $i$, the minimum of
\[
\min_{\mathbf{\mathbf{\xi}}\in\mathcal{X}_{j}}f(\mathbf{x}_{1}^{(k+1)},\cdots,\mathbf{x}_{j-1}^{(k+1)},\mathbf{\xi},\mathbf{x}_{j+1}^{(k)},\cdots,\mathbf{x}_{m}^{(k)})
\]
is uniquely attained. Let $\left\lbrace
\mathbf{x}^{(i)}\right\rbrace $ be the sequence generated by the
block coordinate descent method as in Eq.~\eqref{eq:bcd-method}.
Then, every limit point of $\left\lbrace
\mathbf{x}^{(i)}\right\rbrace $ is a stationary point. The
uniqueness of the minimum is not required for the case when  $m=2$
\cite{GS2000}.
\end{theorem}

The proof of this theorem for an arbitrary number of blocks is shown
in Bertsekas~\cite{Bertsekas1999}.
For a non-convex optimization problem, most algorithms only guarantee
the stationarity of a limit point \cite{Lin2007}.

When applying the BCD method to a constrained non-linear programming
problem, we have to ensure the following (a) it is critical to wisely choose a partition of
$\mathcal{X}$, whose Cartesian product constitutes $\mathcal{X}$. 
(b)  the dependency among the subproblems -- independent blocks can be 
computed simultaneously  (c) when updating a block, ensure we are 
using the most recent other blocks and (d) importantly, every block
must be solved optimally. Every NTF variant will have different the partition
scheme for blocks with natural partition such as scalar \cite{SL2001}, vector \cite{CP2009} and matrix \cite{KP2011}. 
Also, the update equation for determining the optimal block given
other blocks such as projected gradient, block principal pivoting \cite{KP2011}  etc. 
A detailed discussion of equivalences and comparison of different NMF and NTF algorithm
is presented at \cite{KHP2014}. 

In the rest of the section, we will discuss the update equation for different 
algorithms, given the local matrix $\Mn{S}{n}$ and $\Mn{M}{n}_{\V{p}}$ as described in the
Algorithm \ref{alg:2D}. In a similar direction, The paper $MPIFAUN$ \cite{KBP2018}, have detailed some of 
these algorithms on their paper towards NMF.

\subsubsection{Hierarchical Alternating Least Squares (HALS)}
\label{sec:HALS}

In the case of HALS \cite{CP2009}, updates are performed on individual columns of $\HH$ with all other entries in the factor matrices fixed.
The update rules \cite[Algorithm 2]{CP2009} can be written in closed form:
\SplitN{\label{eqn:halsupdate}} {
\Mn[\hat]{H}{n}_{\V{p}}(:,i) &\leftarrow \lt[ \Mn{H}{n}_{\V{p}}(:,i) + \Mn{M}{n}_{\V{p}}(:,i) - (\Mn{H}{n}_{\V{p}} \Mn{S}{n})(:,i)  \rt]_+\\
% \ww^i &\leftarrow \lt[ \ww^i + (\AA\HH^T)^i - \WW (\HH \HH^T)^i \rt]_+
}, 

where, $\Mn{H}{n}_{\V{p}}(:,i) \in \Rn{I_n/P}$ is the $i^{th}$ column of the factor matrix $\Mn{H}{n}_{\V{p}}$.
Note that the columns of $\Mn{H}{n}_{\V{p}}$ are updated in order, so that the most up-to-date values are always used, and these $nk$ updates can be done in an arbitrary order.  In our case we are updating all the $k$ vectors of a factor $\Mn{H}{n}_{\V{p}}$ one by one. 

\subsection{Alternating Nonnegative Least Squares with Block Principal Pivoting}
\label{sec:BPP}

Kim and Park \cite{KP2011} first proposed the algorithm and the detail of the BPP algorithm for NTF is explained subsequently in detailed at \cite{KHP2014}. In this paper, we will leverage it to solve the factors $\Mn{H}{n}$. 
Block Principal Pivoting (BPP) is an active-set-like method for solving the NLS subproblems in Eq. \eqref{eqn:single NLS}.
\SplitN{\label{eqn:single NLS}}{
\min_{\xx\geq 0} \|\CC\XX-\BB\|_2.
}. 
Kannan, Ballard and Park \cite{KBP2018}, have explained solving the active-set based non-negative least squares 
using normal equations with $\CC^T\CC$ and $\CC^T\BB$ for determining the non-negative factors of NMF. 
In our case, $\CC^T\CC$ is $\Mn{S}{n}$ and $\CC^T\BB$ is $\Mn{M}{n}_{\V{p}}$. We use the same implementation
of BPP from $MPIFAUN$.

Our paper can be extended to other NTF algorithms such as Alternating Optimization and Alternating Direction Method of Multipliers (AO-ADMM) \cite{HSL2015, SBK2017} and Nestrov based updates
\cite{LKLHS2017} by changing the $Local-NLS-Update$ function of the proposed Algorithm \ref{alg:2D}. 