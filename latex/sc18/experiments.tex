% !TEX root = paper.tex

\section{Performance Results} \label{sec:experiments}

\input{plots}

\subsection{Datasets}

\subsubsection{Hyperspectral Images (HSI)}
For comparison with previous work \cite{LK+17b}, we consider the same 3D hyperspectral imaging dataset called ``Souto\_wood\_pile'' \cite{FAN16}. 
NNCP is often used on HSI data sets for classification and blind source separation of materials with differing spectral signatures.
The hyperspectral datacube has dimensions $1024 \times 1344 \times 33$ and represents a set of 33 grayscale images of size 1344 $\times$ 1024 pixels sampled at wavelengths 400, 410, $\dots$, 720 nm, with each pixel value representing spectral radiance in $W m^{-2} sr^{-1} nm^{-1}$. 
We also consider the Nogueir\'{o} scene dataset, which is a sequence of 9 time-lapse HSI images of the same scene acquired at about 1-hour intervals.
In each scene, hyperspectral images were acquired at about 1-hour intervals. 
Each Nogueir\'{o} scene HSI image has the same properties as the Souto\_wood\_pile data set, so the corresponding tensor has dimensions $1024 \times 1344 \times 33 \times 9$. 

\subsubsection{Dynamic Functional Connectivity (dFC)}
We also consider dynamic functional connectivity datasets that are generated from fMRI images of human brains.
Given a 4D fMRI data set of voxel measurements across multiple timesteps, voxels containing brain data are partitioned into a set of regions of interest (specified using domain-specific knowledge), and a single time-series signal is aggregated for each region of interest.
Then, an instantaneous correlation is computed for each time point and pair of regions, and this process is repeated for a number of subjects.
Computing a CP decomposition of this data helps to discover patterns of brain connectivity among different regions and also differentiate among individuals.
For our representative dFC data set, we consider 246 brain regions, which yields 30{,}012 unique pairs of regions, 1200 times steps, and 500 subjects, or a tensor of dimension $30{,}012\times 1200 \times 500$ \cite{VEU+12,THBGW17}.

\subsubsection{Synthetic}
Our synthetic data sets are constructed from a CP model with an exact low rank with no added noise.
In this case we can confirm that the residual error of our algorithm with a random start converges to zero.
For the purposes of benchmarking, we run a fixed number of iterations of the BCD algorithm rather than using a convergence check.

%For the synthetic datasets, our open source code supports (a) low-rank tensor (b) uniform random and (c) positive shifted normal distribution of $\mathfrak{N}(3,1)$ -- that is change the mean such that all the random numbers are positive. We considered three different synthetic matrices for different cases. For baseline comparison with Liavas \cite{LK+17b}, we considered a three mode 
%uniform of size 1024x1024x1024 on processor grids $2^k \times 2^k \times 2^k$ for $k \in {0,1,2,3}$. We used the uniform five 
%mode synthetic tensor with dimension $64\times 64\times 64\times 64\times 64$ on processor grids 
%$1\times1\times1\times1\times1$, $2\times1\times1\times1\times1$, $\dots$, $2\times2\times2\times2\times2$ for strong scaling 
%experiments.  In the case of weak scaling of four mode synthetic tensors with (D) and without (N) the use of dimension trees.  The 
%tensor and processor grid dimensions are $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$. In all the cases the dimensions were considered such that synthetic tensors can be accommodated even on single node with 64GB for scale up plots. 

\subsection{Machine Details}
The entire experimentation was performed on Eos, a supercomputer at the Oak Ridge Leadership Computing Facility. 
Eos is a 736-node Cray XC30 cluster of Intel Xeon E5-2670 processors with a total of 47.104TB of memory. 
Its compute nodes are organized in blades where each blade contains 4 nodes, and every node has 2 sockets with 8 physical cores and 64GB memory. 
The machine support Intel's hyper-threading (HT), but we restricted it because HT offers minimal improvement for BLAS and LAPACK operations. 
In total, the Eos compute partition contains 11,776 traditional processor cores and our experiments used up to 8,192 cores (70\% of the machine).
\grey{add compiler and blas details}

\subsection{Comparison Implementations}
The implementation proposed by Liavas et al. \cite{LK+17b} is the only publicly available distributed-memory software (of which we are aware) for computing the CP decomposition of dense tensors, with or without constraints.
We use the acronym NbAO-NTF for Nesterov-based Alternating Optimization Nonnegative Tensor Factorization to refer to their code.

It is based on the same parallel algorithm as our implementation, though it is limited to 3D tensors.
The code uses MPI collectives for communication and Eigen as an interface to BLAS and LAPACK.
We compiled the code linked to OpenBLAS (the same BLAS implementation used by our code) but we were unable to run multithreaded BLAS with their code.
For fair comparison, we use a flat MPI configuration (one MPI process per core) on all comparisons between the two implementations.

We also point out a difference between the Nesterov-based algorithm and the BPP algorithm for solving the NLS subproblems.
The Nesterov-based algorithm attempts an acceleration step using a linear combination of the current and proposed future step; however, it re-computes the residual error before deciding whether or not to accept or reject the acceleration step.
This residual error cannot always be computed cheaply, using the technique described in \cref{sec:error}, and it contributes significantly (approximately 25\%) to the overall run time.
Because the BPP algorithm does not require this extra computation, and studying convergence behavior of the different NLS algorithms is beyond the scope of this work, we remove the time spent in the acceleration step of NbAO-NTF in all our comparisons.

Our proposed algorithm uses dimension trees, but we also benchmark our implementation without that optimization to highlight its importance.
We use an existing implementation to perform the individual MTTKRPs \cite{HBJT18} with this approach.

\subsection{Strong Scaling}

We perform two strong scaling experiments to compare performance with NbAO-NTF.
The experiments use a cubical synthetic tensor and the HSI image used in \cite{LK+17b}, both of which are 3D.

The performance on the cubical synthetic tensor is shown in \cref{fig:strongsynthetic3D}. 
We can observe from the figure that all the three algorithms scale nearly linearly as the problem remains compute bound. 
Recall from \label{sec:analysis} that the computation scales linearly with $1/P$ while the communication scales with $1/P^{1/N}=1/P^{1/3}$. 
As is evident from the figure, the communication cost does not degrade performance even for thousands of cores. 
Our proposed algorithm with dimension trees is 35\% faster than NbAO-NTF at 512 cores (with similar relative difference for other core counts).
This performance improvement is due in large part to the 50\% reduction in arithmetic operations provided by the dimension tree optimization.
There is little difference in performance between our implementation without dimension trees and NbAO-NTF.

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_3D_syn.dat}
\renewcommand{\numiterations}{42}
\liavastrue
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 3D synthetic tensor with dimension $1024\times 1024\times 1024$ on processor grids $2^k\times 2^k\times 2^k$ for $k\in\{0,1,2,3\}$.  The rank is fixed at 32.}
\label{fig:strongsynthetic3D}
\end{figure}

\Cref{fig:stronghsi3D} shows the strong scaling on the HSI data. 
In this case, our proposed algorithm with dimension trees is over $2\times$ faster than NbAO-NTF, but part of this speedup is due to differences in the NLS update algorithms.
For the low core count, the dimension tree provides a 60\% speedup compared to the MTTKRP time in NbAO-NTF.
At the high core counts for this experiment, the local MTTKRP is no longer the dominating cost.
%The proposed algorithm with classical MTTKRP was performing better on higher number of processors over the baseline. 
%Similarly, at this stage, the SVD used for Nestrov Non-negative Least Squares on Liavas dominates the cost over MTTKRP that makes it even slower than the our proposed algorithm without dimension trees. 

Finally, we ran on 512 nodes, utilizing the 16 threads per node via parallel BLAS and OMP, for a $1024\times1024\times1024$ synthetic tensor and processor grid $8\times8\times8$. Here we observe a $253\times$ speedup which is the result of each node having less than 20 megabytes of data.

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_3D_HSI.dat}
\renewcommand{\numiterations}{10}
\liavastrue
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 3D HSI real world data with dimension 1024 x1344 x 33 on processor grids of $1 \times k \times k$ for $k \in {1, 2, 4, 8, 16, 32}$. The rank is fixed at 32.}
\label{fig:stronghsi3D}
\end{figure}
 
In \cref{fig:strongsynthetic5D}, we benchmark performance for a 5D cubical tensor with each dimension set to 64.
Because the tensor is 5D, we can no longer compare against NbAO-NTF.
We see a $13-16\times$ speed up using a dimension tree over not using one for this problem.
As predicted, the dimension tree optimization saves relatively more arithmetic for higher-order tensors.
However, the reduction in leading order flop cost is only $2.5\times$ for $N=5$; the rest of the speedup comes from avoiding  unnecessary KRP computation that is memory bound.  
That is, although the flop count of KRP computation is lower order, it still contributes to the run time because it is inefficient.
Also, for tensors with balanced dimensions, the dimension tree approach has some advantage with favorable shapes for BLAS's DGEMM.


\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_5D_syn.dat}
\renewcommand{\numiterations}{10}
\liavasfalse
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 5D synthetic tensor with dimension $64\times 64\times 64\times 64\times 64$ on processor grids $1\times1\times1\times1\times1$, $2\times1\times1\times1\times1$, $\dots$, $2\times2\times2\times2\times2$.  The rank is fixed at 32.}
\label{fig:strongsynthetic5D}
\end{figure}

\subsection{Weak Scaling Time Breakdown}

We also perform a weak scaling experiment to understand the time it takes to solve bigger problems with more processors.
In this experiment, we use a synthetic 4D tensor and keep the amount of tensor data assigned to each processor constant, with tensor and processor grid of dimensions $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$ and the rank fixed at 32. 
%For eg., in the case of 3 x 3 x 3 x 3 processor grid with 81 nodes, we ran the proposed algorithm with and without dimension trees on a four mode tensor of size 384 x 384 x 384 x 384. 
The results of the breakdown plot is shown in \cref{fig:weaksynthetic4D}. 
In this case, the algorithm is compute bound with and without the use of the dimension tree, so the total time of the weak scaling remains fixed for both cases. 
Using a dimension tree, the time is completely dominated by the MTTKRP computation.  
Without using a dimension tree, we observe that the KRP is expensive and yields a $2.5\times$ slower total run time even in the 4D case. 

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/wk_4D_syn.dat}
\renewcommand{\numiterations}{10}
\breakdownplot
%\labels
\end{tikzpicture}
\caption{Weak scaling of 4D synthetic tensors with (D) and without (N) the use of dimension trees.  The tensor and processor grid dimensions are $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$, and the rank is fixed at 32.  The reported times are per iteration.}
\label{fig:weaksynthetic4D}
\end{figure}

\subsection{Varying Approximation Rank}

\begin{itemize}
	\item 4D HSI (2 algs)
	\item 3D neuroscience (2 algs)
\end{itemize}

\begin{figure}
\renewcommand{\datafile}{data/ksw_4D_HSI.dat}
\renewcommand{\numiterations}{10}
\begin{tikzpicture}
\begin{axis}[	
	ybar stacked,
	bar width=8pt,
	width=\columnwidth,
	height =.75\columnwidth,
	%width=9cm, height=3.85cm,
	ylabel={Time (s)}, 
	xlabel={Rank $k$},
	y label style={yshift=-.5cm},
	ymin=0,
	%symbolic x coords={D10,N10,,D20,N20,,D30,N30,,D40,N40,,D50,N50},
	symbolic x coords={D10,D20,D30,D40,D50},
	%xtick=data,
	xticklabels={,10,20,30,40,50},
	legend style={at={(0.5,1.3)},anchor=north},
	legend columns=-1,
]
	\setcolors
	\addplot table[x=alg-K, y expr=(\thisrow{mttkrp}/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=(\thisrow{mttv}/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=((\thisrow{nnls}+\thisrow{gram})/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=((\thisrow{allgather}+\thisrow{reducescatter})/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=(\thisrow{allreduce}/(\minvalue*\numiterations))] {\datafile};
	\legend{PM,mTTV,NLS,Factor Comm,Gram Comm};
\end{axis}
%\labels
\end{tikzpicture}
\caption{Per-iteration time breakdown of our implementation (using dimension trees) over various ranks for a time-lapse HSI dataset with dimensions $1344\times 1024\times 33 \times 9$ on 64 processors arranged in a $8\times8\times1\times1$ grid.}
\label{fig:ksweep4DHSI}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[legend style={draw=none, anchor=north,  cells={align=left}, nodes={scale=0.7}}, legend pos=north west]
	\addplot+ [discard if not={alg}{D},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\addplot+ [discard if not={alg}{N},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\addplot+ [discard if not={alg}{L},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\legend{DimTree,NoDimTree,Liavas}
\end{axis}
\end{tikzpicture}
\caption{$k$ sweep plot for Neuroscience dataset}
\label{fig:ksweepneuro}
\end{figure}

According to Kim, He and Park \cite{KHP2014}, in the case of dense tensors, as the low rank $k$ increases, the approximation 
error  $\|\TA - \T{M}\|$ decreases because it is obtaining bigger low rank factors. However, there is a law of diminishing advantages, 
that an increase in $k$ in lower values, say from 5 to 10, will have significant improvement in relative error over increase in higher 
values, like 100 to 105. Hence, it is common practice in the community to sweep $k$, to obtain better approximation error within the 
manageable computation. Towards this end, we wanted to understand the increase in running time over increasing the low rank $k
$. The results of the experiments on real world four mode HSI tensor is presented in Figure \ref{fig:ksweep4DHSI}. We can observe 
that, the multi-TTV({\em mTTV}) scales linearly with the increasing $k$,  where as the partial MTTKRP ({\em PM}) is scaling super 
linearly. This is because, with higher $k$, the BLAS $DGEMM$ gets a bigger chunk of work that it can optimize better.  The local 
compute $NLS$ is increasing as expected nearly $O(k^3)$ and the $ALL-GATHER$ required for the gram matrices over $k^2$ 
elements, becomes significant on higher $k$'s. 

In Figure \ref{fig:ksweepneuro} we compare k-sweeps for $R = 10,20,...,50$ for all 3 algorithms. Starting at $R=10$ we see the largest speed up of $2\times$ for dimension trees over Liavas. This is due to a combination of the dimension tree performing less flops in the MTTKRPs and KRPs. However, as the rank increases this speed up diminishes to $1.6\times$. The loss of speed up is a result of the fact that, as we observed in Figure \ref{fig:ksweep4DHSI}, the multi-TTV operations do not scale as well as the partial-MTTKRPs for increasing $R$. 

%\begin{figure}

% set which run to plot
%\renewcommand{\run}{3}
%
%\begin{subfigure}{0.3 \columnwidth}
%\ylabeltrue
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/denserwerr-time.dat}
%\renewcommand{\run}{1}
%\relerrplot
%\renewcommand{\run}{3}
%\end{tikzpicture}
%\ylabelfalse
%\subcaption{Video}
%\label{fig:denserwerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/stkx-5runs-err.dat}
%\legendtrue
%\relerrplot
%\legendfalse
%\end{tikzpicture}
%\subcaption{Stack Exchange}
%\label{fig:stackexchangeerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/webbase1M-5runs-err.dat}
%\relerrplot
%\end{tikzpicture}
%\subcaption{Webbase}
%\label{fig:sparserwerr}
%\end{subfigure}
%
%\caption{Relative error comparison of \MU, \HALS, \BPP\ on real world datasets.}
%\label{fig:convergence}
%\end{figure}

% set which run to plot
%\renewcommand{\run}{3}
%
%\begin{subfigure}{0.3 \columnwidth}
%\ylabeltrue
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/denserwerr-time.dat}
%\renewcommand{\run}{1}
%\strongscalingplot
%\renewcommand{\run}{3}
%\end{tikzpicture}
%\ylabelfalse
%\subcaption{Video}
%\label{fig:denserwerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/stkx-5runs-err.dat}
%\legendtrue
%\strongscalingplot
%\legendfalse
%\end{tikzpicture}
%\subcaption{Stack Exchange}
%\label{fig:stackexchangeerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/webbase1M-5runs-err.dat}
%\strongscalingplot
%\end{tikzpicture}
%\subcaption{Webbase}
%\label{fig:sparserwerr}
%\end{subfigure}
%
%\caption{Relative error comparison of \MU, \HALS, \BPP\ on real world datasets.}
%\label{fig:convergence}
%\end{figure}