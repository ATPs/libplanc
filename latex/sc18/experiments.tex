% !TEX root = paper.tex

\section{Performance Results} \label{sec:experiments}

\input{plots}

\subsection{Datasets}

\subsubsection{Hyperspectral Images (HSI)}
For comparison with previous work \cite{LK+17b}, we consider the same 3D hyperspectral imaging dataset called ``Souto\_wood\_pile'' \cite{FAN16}. 
The hyperspectral datacube has dimensions $1024 \times 1344 \times 33$ and represents a set of 33 grayscale images of size 1344 $\times$ 1024 pixels sampled at wavelengths 400, 410, $\dots$, 720 nm, with each pixel value representing spectral radiance in $W m^{-2} sr^{-1} nm^{-1}$. 
We also consider the Nogueir\'{o} scene dataset, which is a sequence of 9 time-lapse HSI images of the same scene acquired at about 1-hour intervals.
In each scene, hyperspectral images were acquired at about 1-hour intervals. 
Each Nogueir\'{o} scene HSI image has the same properties as the Souto\_wood\_pile data set, so the corresponding tensor has dimensions $1024 \times 1344 \times 33 \times 9$. 

\subsubsection{Dynamic Functional Connectivity (dFC)}



\subsubsection{Synthetic}
For the synthetic datasets, our open source code supports (a) low-rank tensor (b) uniform random and (c) positive shifted normal distribution of $\mathfrak{N}(3,1)$ -- that is change the mean such that all the random numbers are positive. We considered three different synthetic matrices for different cases. For baseline comparison with Liavas \cite{LK+17b}, we considered a three mode 
uniform of size 1024x1024x1024 on processor grids $2^k \times 2^k \times 2^k$ for $k \in {0,1,2,3}$. We used the uniform five 
mode synthetic tensor with dimension $64\times 64\times 64\times 64\times 64$ on processor grids 
$1\times1\times1\times1\times1$, $2\times1\times1\times1\times1$, $\dots$, $2\times2\times2\times2\times2$ for strong scaling 
experiments.  In the case of weak scaling of four mode synthetic tensors with (D) and without (N) the use of dimension trees.  The 
tensor and processor grid dimensions are $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$. In all the cases the dimensions were considered such that synthetic tensors can be accommodated even on single node with 64GB for scale up plots. 

\subsection{Machine Details}
The entire experimentation was performed on Eos -- a super computer in Oak Ridge Leadership Computing Facility (OLCF). 
Eos is a 736-node Cray® XC30? cluster with a total of 47.104TB of memory. Its processor is the Intel® Xeon® E5-2670. It features 
16 I/O service nodes and 2 external login nodes. Its compute nodes are organized in blades. Each blade contains 4 nodes and every node has 2 sockets with 8 physical cores and 64GB memory. Even though the machine support Intel's hyper-threading (HT), we restricted the total number of OpenMP threads to match the number of physical cores as HT offers minimal improvement for
 BLAS and LAPACK operations. In total, the Eos compute partition contains 11,776 traditional processor cores (23,552 logical cores 
 with HT Technology enabled) and some of the experiments peaked to 8,192 out of 11,776 physical cores that is approximately 
 70\% of the peak capacity.

\subsection{Strong Scaling}

\begin{itemize}
	\item 3D HSI (3 algs)
	\item 3D synthetic (3 algs)
	\item 5D synthetic (2 algs)
\end{itemize}

The strong scaling on 3D synthetic tensor with baseline is shown in Figure \ref{fig:strongsynthetic3D}. 
We can observe from the figure that all the three algorithms scale linearly as the problem is more
compute bound in the case of dense tensors. The computation is dominated by $O(R \prod I_k / P_k)$
and the communication scales as $P^{1/N}$. As evident from the figure, this communication cost doesn't 
show up even for $P$ in the order of few hundreds. Our proposed algorithm with dimension trees is 1.5x faster than the baseline 
{\em Liavas}\cite{LK+17b}. 
The dimension trees are faster over classical MTTKRP, in both the three and five mode synthetic tensors. We are witnessing
2.5x speed up with dimension trees over higher modes as in Figure \ref{fig:strongsynthetic5D}. 
The advantages of the dimension tree are more pronounced in higher
mode tensors because of more reuse over partial MTTKRPs and avoiding unnecessary KRP computation that is more memory bound.  Also, for tensors with equal dimensions over all the
modes, dimension trees has some advantage with favorable shapes for matrix multiplication with BLAS DGEMM.

The Figure \ref{fig:stronghsi3D} shows the strong scaling on the three modes HSI real world data. We can observe
that our proposed algorithm with dimension trees is almost 2x faster than baseline Liavas. The proposed algorithm
with classical MTTKRP was performing better on higher number of processors over the baseline. 

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_3D_syn.dat}
\renewcommand{\numiterations}{42}
\liavastrue
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 3D synthetic tensor with dimension $1024\times 1024\times 1024$ on processor grids $2^k\times 2^k\times 2^k$ for $k\in\{0,1,2,3\}$.  The rank is fixed at 32.}
\label{fig:strongsynthetic3D}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_5D_syn.dat}
\renewcommand{\numiterations}{10}
\liavasfalse
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 5D synthetic tensor with dimension $64\times 64\times 64\times 64\times 64$ on processor grids $1\times1\times1\times1\times1$, $2\times1\times1\times1\times1$, $\dots$, $2\times2\times2\times2\times2$.  The rank is fixed at 32.}
\label{fig:strongsynthetic5D}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/str_3D_HSI.dat}
\renewcommand{\numiterations}{10}
\liavastrue
\strongscalingplot
\end{tikzpicture}
\caption{Strong scaling of 3D HSI real world data with dimension 1024 x1344 x 33 on processor grids of $1 \times k \times k$ for $k \in {1, 2, 4, 8, 16, 32}$. The rank is fixed at 32.}
\label{fig:stronghsi3D}
\end{figure}

\subsection{Weak Scaling}

\begin{itemize}
	\item 4D synthetic (2 algs)
	\item 3D neuroscience, increasing one mode? (2 algs)
\end{itemize}

In the case of weakscaling, we wanted to understand the time it takes to solve bigger problems by increasing the number of nodes with the increase in data size by keeping the problem size (workload) assigned to each processing element constant. We conducted this experiment by using synthetic tensor and processor grid of dimensions $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$, and the rank is fixed at 32. For eg., in the case of 3 x 3 x 3 x 3 processor grid with 81 nodes, we ran the proposed algorithm with and without dimension trees on a four mode tensor of size 384 x 384 x 384 x 384. The results of the breakdown plot is shown in Figure \ref{fig:weaksynthetic4D}. Since, the NTF on dense tensors is compute intensive, the total time of the weak scaling is staying fixed for both the case of with and without dimension trees. The entire running is completely dominated by the MTTKRP computation.  As mentioned above, from the Figure   \ref{fig:weaksynthetic4D}, we can observe that the KRP cost of dimension trees is insignificant and it offers 2.5x speed up over classical MTTKRP. 

\begin{figure}
\begin{tikzpicture}
\renewcommand{\datafile}{data/wk_4D_syn.dat}
\renewcommand{\numiterations}{10}
\breakdownplot
%\labels
\end{tikzpicture}
\caption{Weak scaling of 4D synthetic tensors with (D) and without (N) the use of dimension trees.  The tensor and processor grid dimensions are $128k\times 128k\times 128k\times 128k$ and $k\times k\times k\times k$ for $k\in\{1,2,3,4\}$, and the rank is fixed at 32.  The reported times are per iteration.}
\label{fig:weaksynthetic4D}
\end{figure}

\subsection{Varying Approximation Rank}

\begin{itemize}
	\item 4D HSI (2 algs)
	\item 3D neuroscience (2 algs)
\end{itemize}

\begin{figure}
\renewcommand{\datafile}{data/ksw_4D_HSI.dat}
\renewcommand{\numiterations}{10}
\begin{tikzpicture}
\begin{axis}[	
	ybar stacked,
	bar width=8pt,
	width=\columnwidth,
	height =.75\columnwidth,
	%width=9cm, height=3.85cm,
	ylabel={Time (s)}, 
	xlabel={Rank $k$},
	y label style={yshift=-.5cm},
	ymin=0,
	%symbolic x coords={D10,N10,,D20,N20,,D30,N30,,D40,N40,,D50,N50},
	symbolic x coords={D10,D20,D30,D40,D50},
	%xtick=data,
	xticklabels={,10,20,30,40,50},
	legend style={at={(0.5,1.3)},anchor=north},
	legend columns=-1,
]
	\setcolors
	\addplot table[x=alg-K, y expr=(\thisrow{mttkrp}/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=(\thisrow{mttv}/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=((\thisrow{nnls}+\thisrow{gram})/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=((\thisrow{allgather}+\thisrow{reducescatter})/(\minvalue*\numiterations))] {\datafile};
	\addplot table[x=alg-K, y expr=(\thisrow{allreduce}/(\minvalue*\numiterations))] {\datafile};
	\legend{PM,mTTV,NLS,Factor Comm,Gram Comm};
\end{axis}
%\labels
\end{tikzpicture}
\caption{Per-iteration time breakdown of our implementation (using dimension trees) over various ranks for a time-lapse HSI dataset with dimensions $1344\times 1024\times 33 \times 9$ on 64 processors arranged in a $8\times8\times1\times1$ grid.}
\label{fig:ksweep4DHSI}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[legend style={draw=none, anchor=north,  cells={align=left}, nodes={scale=0.7}}, legend pos=north west]
	\addplot+ [discard if not={alg}{D},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\addplot+ [discard if not={alg}{N},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\addplot+ [discard if not={alg}{L},thick,mark options={solid},mark=square*] table [x={k}, y expr=(\thisrow{total}/(\minvalue*\numiterations))] {data/kswp_neuro.dat};
	\legend{DimTree,NoDimTree,Liavas}
\end{axis}
\end{tikzpicture}
\caption{$k$ sweep plot for Neuroscience dataset}
\label{fig:ksweepneuro}
\end{figure}

According to Kim, He and Park \cite{KHP2014}, in the case of dense tensors, as the low rank $k$ increases, the approximation 
error  $\|\TA - \T{M}\|$ decreases because it is obtaining bigger low rank factors. However, there is a law of diminishing advantages, 
that an increase in $k$ in lower values, say from 5 to 10, will have significant improvement in relative error over increase in higher 
values, like 100 to 105. Hence, it is common practice in the community to sweep $k$, to obtain better approximation error within the 
manageable computation. Towards this end, we wanted to understand the increase in running time over increasing the low rank $k
$. The results of the experiments on real world four mode HSI tensor is presented in Figure \ref{fig:ksweep4DHSI}. We can observe 
that, the multi-TTV({\em mTTV}) scales linearly with the increasing $k$,  where as the partial MTTKRP ({\em PM}) is scaling super 
linearly. This is because, with higher $k$, the BLAS $DGEMM$ gets a bigger chunk of work that it can optimize better.  The local 
compute $NLS$ is increasing as expected nearly $O(k^3)$ and the $ALL-GATHER$ required for the gram matrices over $k^2$ 
elements, becomes significant on higher $k$'s. 

%\begin{figure}

% set which run to plot
%\renewcommand{\run}{3}
%
%\begin{subfigure}{0.3 \columnwidth}
%\ylabeltrue
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/denserwerr-time.dat}
%\renewcommand{\run}{1}
%\relerrplot
%\renewcommand{\run}{3}
%\end{tikzpicture}
%\ylabelfalse
%\subcaption{Video}
%\label{fig:denserwerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/stkx-5runs-err.dat}
%\legendtrue
%\relerrplot
%\legendfalse
%\end{tikzpicture}
%\subcaption{Stack Exchange}
%\label{fig:stackexchangeerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/webbase1M-5runs-err.dat}
%\relerrplot
%\end{tikzpicture}
%\subcaption{Webbase}
%\label{fig:sparserwerr}
%\end{subfigure}
%
%\caption{Relative error comparison of \MU, \HALS, \BPP\ on real world datasets.}
%\label{fig:convergence}
%\end{figure}

% set which run to plot
%\renewcommand{\run}{3}
%
%\begin{subfigure}{0.3 \columnwidth}
%\ylabeltrue
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/denserwerr-time.dat}
%\renewcommand{\run}{1}
%\strongscalingplot
%\renewcommand{\run}{3}
%\end{tikzpicture}
%\ylabelfalse
%\subcaption{Video}
%\label{fig:denserwerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/stkx-5runs-err.dat}
%\legendtrue
%\strongscalingplot
%\legendfalse
%\end{tikzpicture}
%\subcaption{Stack Exchange}
%\label{fig:stackexchangeerr}
%\end{subfigure}
%~
%\begin{subfigure}{0.3 \columnwidth}
%\begin{tikzpicture}
%\renewcommand{\datafile}{data/webbase1M-5runs-err.dat}
%\strongscalingplot
%\end{tikzpicture}
%\subcaption{Webbase}
%\label{fig:sparserwerr}
%\end{subfigure}
%
%\caption{Relative error comparison of \MU, \HALS, \BPP\ on real world datasets.}
%\label{fig:convergence}
%\end{figure}