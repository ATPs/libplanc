\section{Conclusion}\label{sec:conclusion}

In this paper, we propose a high-performance distributed-memory parallel framework for NMF algorithms that iteratively update the low rank factors in an alternating fashion.   
Our parallelization scheme is designed to avoid communication overheads and scales well to over 1500 cores. 
The framework is flexible, being (a) expressive enough to leverage many different NMF algorithms and (b) efficient for both sparse and dense matrices of sizes that span from a few hundreds to hundreds of millions.  
Our open-source software implementation is available for download.

%For the data sets on which we experimented, we showed that an efficient implementation of a naive parallel algorithm spends much of its time in interprocessor communication.
%For sparse problems, comparing p = 16 to p = 1536 (a factor increase of 96), we observe a speedups from \BPP\ of 59× (synthetic) and 22× (real world). For dense problems, comparing p = 96 to p = 1536 (a factor increase of 16), \BPP's speedup is 12× for both problems. The other NMF algorithms -- \MU\ and \HALS\, demonstrate similar scaling results in our framework.
For solving data mining problems at today's scale, parallel computation and distributed-memory systems are becoming prerequisites.
We argue in this paper that by using techniques from high-performance computing, the computations for NMF can be performed very efficiently.
Our framework allows for the HPC techniques (efficient matrix multiplication) to be separated from the data mining techniques (choice of NMF algorithm), and we compare data mining techniques at large scale, in terms of data sizes and number of processors.
One conclusion we draw from the empirical and theoretical observations is that the extra per-iteration cost of \BPP\ over alternatives like \MU\ and \HALS\ decreases as the number of processors $p$ increases, making \BPP\ more advantageous in terms of both quality and performance at larger scales.
By reporting time breakdowns that separate local computation from interprocessor communication, we also see that our parallelization scheme prevents communication from bottlenecking the overall computation; our comparison with a naive approach shows that communication can easily dominate the running time of each iteration. 
%In the case of \ParNMF, the problems remain computation bound on up to 600 processors, typically spending most of the time in local matrix multiplication or NLS solves.

%We focus in this work on BPP, %which is more expensive per-iteration than alternative methods like MU and HALS, 
%because it has been shown to reduce overall running time in the sequential case by requiring fewer iterations \cite{kim2011fast}.
%Because much of the time per iteration of \ParNMF\ is spent on local NLS, we believe further empirical exploration is necessary 
%to understand the proposed \ParNMF{}'s advantages for other AU-NMF algorithms such as MU and HALS. 
%%to confirm the advantages of BPP in the parallel case.
%We note that if we use the MU or HALS approach for determining low rank factors, the relative cost of interprocessor communication will grow, making the communication efficiency of our algorithm more important.

In future work, we would like to extend \ParNMF\  algorithm to dense and sparse tensors, computing the CANDECOMP/PARAFAC decomposition in parallel with non-negativity constraints on the factor matrices.
We plan on extending our software to include more NMF algorithms that fit the AU-NMF framework; these can be used for both matrices and tensors. 
We would also like to explore more intelligent distributions of sparse matrices: while our 2D distribution is based on evenly dividing rows and columns, it does not necessarily load balance the nonzeros of the matrix, which can lead to load imbalance in matrix multiplications.
We are interested in using graph and hypergraph partitioning techniques to load balance the memory and computation while at the same time reducing communication costs as much as possible.
%Finally, based our survey, experiments and knowledge, we would like to conclude that, so far we are the fastest distributed NMF algorithm in the literature. 
%Finally, we have not yet reached the limits of the scalability of \ParNMF; we would like to expand our benchmarks to larger numbers of nodes on the same size data sets to study performance behavior when communication costs completely dominate the running time.
