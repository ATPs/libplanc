\section{Introduction}

Non-negative Matrix Factorization (NMF) is the problem of finding two low rank factors $\WW\in \Rnplus{m\times k}$ and $\HH\in \Rnplus{k \times n}$ for a given input matrix  $\AA\in \Rnplus{m\times n}$, such that $\AA \approx \WW \HH$.
Here, $\Rnplus{m\times n}$ denotes the set of $m \times n$ matrices with non-negative real values.
Formally, the NMF problem \cite{seung2001algorithms} can be defined as \SplitN{\label{eqn:original NMF}}{
\min_{\WW \geq 0,\HH \geq 0} & \|\AA-\WW\HH\|_F,
}
where $\|\M{X}\|_F=(\sum_{ij} x_{ij}^2)^{1/2}$ is the Frobenius norm.

NMF is widely used in data mining and machine learning as a dimension reduction and factor analysis method. 
It is a natural fit for many real world problems as the non-negativity is inherent in many representations of real-world data and
the resulting low rank factors are expected to have a natural interpretation. The applications of NMF range from text mining \cite{pauca2004text},  computer vision \cite{hoyer2004non,Fujimoto2014,BSJJZ2015}, and bioinformatics \cite{kim2007sparse} to blind source separation  \cite{cichocki2009nonnegative}, unsupervised clustering \cite{kuang2012symmetric,kuang2013symnmf}  and many other areas.
In the typical case, $k \ll \min(m,n)$; for problems today, $m$ and $n$ can be on the order of millions or more, and $k$ is on the order of few tens to thousands.

There is a vast literature
on algorithms for NMF and their convergence properties \cite{kim2013nonnegative}.   
The commonly adopted NMF algorithms are -- (i) Multiplicative Update (\MU) \cite{seung2001algorithms} (ii) Hierarchical Alternating Least Squares (\HALS) \cite{cichocki2009nonnegative,Ho2008} (iii) NMF based on Alternating Nonnegative Least Squares and Block Principal Pivoting (\BPP) \cite{kim2011fast}, and (iv) Stochastic Gradient Descent (SGD) Updates \cite{gemulla2011large}. 
Most of the algorithms in NMF literature are based on alternately optimizing each of the low rank factors $\WW$ and $\HH$ while keeping the other fixed, in which case each subproblem is a constrained convex optimization problem. 
Subproblems can then be solved using standard optimization techniques such as projected gradient or interior point method; a detailed survey for solving such problems can be found in \cite{xiong2013survey,kim2013nonnegative}. 
In this paper, our implementation uses either \BPP, \MU, or \HALS. 
But our parallel framework is extensible to other algorithms (e.g., \cite{XY2013,HSL2016}) as-is or with a few modifications, as long as they fit an alternating-updating framework (defined in Section \ref{sec:aunmf}).  

With the advent of large scale internet data and interest in Big Data, researchers have started studying scalability of many foundational machine learning algorithms. 
To illustrate the dimension of matrices commonly used in the machine learning community, we present a few examples. 
Nowadays the adjacency matrix of a billion-node social network is common. 
In the matrix representation of a video data, every frame contains three matrices for each RGB color, which is reshaped into a column.  
Thus in the case of a 4K video, every frame will take approximately 27 million rows (4096 row pixels x 2196 column pixels x 3 colors). 
Similarly, the popular representation of documents in text mining is a bag-of-words matrix, where the rows are the dictionary and the columns are the documents (e.g., webpages). 
Each entry $A_{ij}$ in the bag-of-words matrix is generally the frequency count of the word $i$  in the document $j$. 
Typically with the explosion of the new terms in social media, the number of words spans to millions. 
To handle such high-dimensional matrices, it is important to study low-rank approximation methods in a data-distributed and parallel computing environment. 

In this work, we present an efficient algorithm and implementation using tools from the field of High-Performance Computing (HPC).
We maintain data in memory (distributed across processors), take advantage of optimized libraries like BLAS and LAPACK for local computational routines, and use the Message Passing Interface (MPI) standard to organize interprocessor communication.
%In particular, we use an MPI/C++ implementation, which offers several technical advantages such as (1) the ability to leverage state-of-the-art hardware, (2) efficient networks for communication between nodes, and (3) the availability of numerically stable and efficient BLAS and LAPACK routines. 
Furthermore, the current hardware trend is that available parallelism (and therefore aggregate computational rate) is increasing much more quickly than improvements in network bandwidth and latency, which implies that the relative cost of communication (compared to computation) is increasing.
To address this challenge, we analyze algorithms in terms of both their computation and communication costs.
In particular, we prove in Section \ref{sec:parNMF} that in the case of dense input and under a mild assumption, our proposed algorithm minimizes the amount of data communicated between processors to within a constant factor of the lower bound.

We call our implementation MPI-FAUN, an MPI-based Framework for Alternating-Updating Nonnegative matrix factorization algorithms.
A key attribute of our framework is that the efficiency does not require a loss of generality of NMF algorithms.
Our central observation is that most NMF algorithms, in particular those that alternate between updating each factor matrix, consist of two main tasks: (a) performing matrix multiplications and (b) solving Non-negative Least Squares (NLS) subproblems, either approximately or exactly.
More importantly, NMF algorithms tend to perform the same matrix multiplications, differing only in how they solve NLS subproblems, and the matrix multiplications often dominate the running time of the algorithms.
Our framework is designed to perform the matrix multiplications efficiently and organize the data so that the NLS subproblems can be solved independently in parallel, leveraging any of a number of possible methods.
We explore the overall efficiency of the framework and compare three different NMF methods in Section \ref{sec:experiment}, performing convergence, scalability, and parameter-tuning experiments on over 1500 processors.

\begin{table}[htp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
Dataset & Type & Matrix size & NMF Time \\ \hline
Video & Dense & 1 Million x 13,824 &  5.73 seconds \\
Stack Exchange & Sparse & 627,047 x 12 Million &  67 seconds \\
Webbase-2001 & Sparse & 118 Million x 118 Million & 25 minutes \\ \hline
\end{tabular}
\end{center}
\caption{\ParNMF\ on large real-world datasets. Reported time is for 30 iterations on 1536 processors with a low rank of 50.}
\label{tab:teaser}
\end{table}%


With our framework, we are able to explore several large-scale synthetic and real-world data sets, some dense and some sparse. In Table \ref{tab:teaser}, we present the NMF computation wall clock time on some very large real world datasets. We describe the results of the computation in Section \ref{sec:experiment}, showing the range of application of NMF and the ability of our framework to scale to large data sets.  

%For example, we apply NMF to a surveillance video data set of size 44 GB in order to isolate the moving objects from the background view; once loaded into memory, the most efficient algorithm requires only 5.73 seconds on 1536 processors.
%We also apply our techniques to sparse data, analyzing Stack Exchange posts (text data) to infer discussion topics; this data set is 44 GB of XML file which takes 6.1 GB of sparse matrix representation, and our NMF implementation requires 67 seconds on 1536 processors to obtain meaningful results 
%Similarly, our algorithm can perform graph clustering using NMF on a very large sparse graph with 118 million nodes and 1 billion edges and the sparse representation of this matrix takes 18GB on disk 
%in nearly 15 minutes using 1536 processors, 
%\grey{do we want to report that the output dense matrices required 88GB?}.
%\grey{break up into multiple sentences}


A preliminary version of this work has already appeared as a conference paper \cite{KBP16}.
While the focus of the previous work was parallel performance of \BPP\ (Alternating Nonnegative Least Squares and Block Principal Pivoting), the goal of this paper is to explore more data analytic questions.
In particular, the new contributions of this paper include (1) implementing a software framework to compare \BPP\ with \MU\ (Multiplicative Update) and \HALS\ (Hierarchical Alternating Least Squares) for large scale data sets, (2) benchmarking on a data analysis cluster and scaling up to over 1500 processors, and (3) providing an interpretation of results for real-world data sets.
We provide a detailed comparison with other related work, including MapReduce implementations of NMF, in Section \ref{sec:related}.

Our main contribution is a new, high-performance parallel computational framework for a broad class of NMF algorithms. 
The framework is efficient, scalable, flexible, and demonstrated to be effective for large-scale dense and sparse matrices.  
Based on our survey and knowledge, we are the fastest NMF implementation available in the literature.  
The code and the datasets used for conducting the experiments can be downloaded from \url{https://github.com/ramkikannan/nmflibrary}. 