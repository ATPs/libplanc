\section{Distributed Sparse NMF}

% Notation for frequently used matrices and indices. Defined in spdistnmfsc17.tex, commented out here for reference.
% \newcommand{\mat}[1]{\mathbf{#1}}  % Denote a matrix
% \newcommand{\matt}[1]{\mathbf{\tilde{#1}}}  % Denote a matrix with hat
% \newcommand{\Am}{\mat{A}} % Input sparse matrix
% \newcommand{\Amp}{\mat{A}_p} % Input sparse matrix
% \newcommand{\Wm}{\mat{W}} % Factor matrix
% \newcommand{\Wmt}{\matt{W}} % AH result
% \newcommand{\Hm}{\mat{H}} % Factor matrix
% \newcommand{\Hmt}{\matt{H}} % WtA result
% \newcommand{\Ip}{\mathcal{I}_p}
% \newcommand{\Jp}{\mathcal{J}_p}
% \newcommand{\Fp}{\mathcal{F}_p}
% \newcommand{\Gp}{\mathcal{G}_p}

Here, we first introduce our parallel NMF algorithm that operates on a partition of the matrices $\Am$, $\Wm$, and $\Hm$.
For a given partition, we describe how parallel computations and communications take place within the algorithm, and illustrate computational and communication costs associated with a partition.
We then discuss efficient partitioning strategies to better establish computational load balance and reduce communication in NMF.
In doing so, we also explain how existing methods compare to this scheme with their advantages and disadvantages in terms of partitioning.

\subsection{Distributed Sparse NMF Algorithm}

\begin{algorithm}
\caption{\distspnmf: \distspnmffull}
\label{alg:distnmf}
  \begin{algorithmic}[1]
    \setcounter{ALC@unique}{0}
    \REQUIRE {$\Amp$: An $m \times n$ sparse matrix \\
      $\Ip$,$\Jp$: Set of rows/columns of $\Wm$ and $\Hm$ owned by process $p$ \\
      $\Fp$,$\Gp$: Footprints of process $p$ on $\Wm$ and $\Hm$ \\
      $\Wm(\Ip,:)$,$\Hm(:,\Jp)$: Owned rows/columns of $\Wm$ and $\Hm$ \\
      $k$: The NMF rank}
    \ENSURE {Process $p$ gets final values of $\Wm(\Ip,:)$ and $\Hm(:,\Jp)$}
      \REPEAT
        \STATE $\Cexpand (\Hm(:,\Gp))$ \label{line:exph}
        \STATE $\Wmt(\Fp, :) \gets \Amp \Hm(:, \Gp)$ \label{line:ah}
        \STATE $\Cfold(\Wmt(\Fp, :))$ \label{line:foldw}
        \STATE $\Hmgr \gets \textsc{All-Reduce}(\Hm(:, \Jp) \Hm(:, \Jp)^T)$ \label{line:hmgr}
        \STATE $\Wm(\Ip, :) \gets \textsc{Nnls}(\Hmgr, \Wmt(\Ip, :))$ \label{line:wmnnls}
        \STATE $\Cexpand(\Wm(\Fp, :))$ \label{line:expw}
        \STATE $\Hmt(:, \Gp) \gets \Wm(\Fp, :)^T \Amp$ \label{line:wta}
        \STATE $\Cfold(\Hmt(:, \Gp))$ \label{line:foldh}
        \STATE $\Wmgr \gets \textsc{All-Reduce}(\Wm(\Ip, :)^T \Wm(\Ip, :))$ \label{line:wmgr}
        \STATE $\Hm(\Jp, :) \gets \textsc{Nnls}(\Wmgr, \Hmt(\Jp, :))$ \label{line:hmnnls}
      \UNTIL{convergence or maximum number of iterations}
  \end{algorithmic}
\end{algorithm}

Parallelizing sparse NMF involves partition the sparse matrix $\Am$ as well as the factor matrices $\Wm$ and $\Hm$, where the former partitioning distributes the computational load of sparse matrix-dense matrix multiplications $\Am \Hm$ and $\Wm^T \Am$, whereas the latter divides the workload of NNLS computations to processes.
We provide the execution of our parallel algorithm for computing a rank-$k$ NMF of a sparse matrix $\Am \in \R^{m \times n}$ in \cref{alg:distnmf}, which is executed by each process $p$ for $1 \le p \le P$.
The algorithm starts with an arbitrary partition of the input matrix and the factor matrices; process $p$ owns the submatrices $\Wm(\Ip, :)$ and $\Hm(:, \Jp)$ as well as the nonzero elements of the sparse matrix $\Am_p$ where $\Am = \bigcup_{i=1}^{P}\Am_i$, i.e., $\Am_1, \dots, \Am_P$ partitions the nonzeros of $\Am$.
The sets $\Fp$ and $\Gp$ denote the ``footprints'' of the process $p$ on the rows and the columns of matrices $\Wm$ and $\Hm$, respectively; hence, these rows need to be stored by this process.
Specifically, we have $i \in \Fp$ or $j \in \Gp$ if only if $i \in \Ip$ or $j \in \Jp$~(row/column is owned), or there is a nonzero element $a_{i,j} \in \Amp$~(row/column is used in local computations).
At each iteration, the process $p$ is responsible for gathering the new value of submatrices $\Wm(\Ip, :)$ and $\Hm(:, \Jp)$, and sending them to processes in need.

In an iteration of \cref{alg:distnmf}, each process $p$ possesses three types of computational tasks as well as associated pre- and post-communication steps.
The first task involves performing sparse matrix-dense matrix multiplications $\Am_p \Hm(:, \Gp)$ and $\Wm(\Fp, :)^T \Am_p$, whose results are stored in distributed matrices $\Wmt$ and $\Hmt$, which follow the same row-/column-wise data distribution as $\Wm$ and $\Hm$.
Note that carrying out these multiplications must be preceded by a communication step where each process $p$ gets the submatrices $\Hm(:, \Gp \setminus \Jp)$ and $\Wm(\Fp \setminus \Ip, :)$ that are accessed by entries of $\Amp$, and this step is performed at \cref{line:exph,line:expw}.
These multiplications performed by each process $p$ generate partial results for the set $\Fp$ and $\Gp$ of rows of $\Wmt$ and columns of $\Hmt$, respectively, which is highlighted at \cref{line:ah,line:wta}.
Indeed, partial results for the submatrices $\Wmt(\Fp \setminus \Ip, :)$ and $\Hmt(:, \Gp \setminus \Jp)$ correspond to rows and columns owned by other processes; hence, they need to be communicated.
The results for $\Wmt(\Ip, :)$ and $\Hmt(:, \Jp)$, however, should be kept locally, and all partial results for these matrix rows and columns generated by other processes should similarly be received and accumulated in order to obtain the final value for these owned portions.
The second task is to compute the Gram matrices $\Hmgr = \Hm \Hm^T$ and $\Wmgr = \Wm^T \Wm$ of size $k \times k$, and making these matrices available to all processes, which is performed at \cref{line:hmgr,line:wmgr}.
This is done in a row-parallel dense matrix multiplication step, in which the process $p$ computes $\Hm(:, \Jp) \Hm^T(:, \Jp)$ and $\Wm^T(\Ip, :) \Wm(\Ip, :)$, followed by an \textsc{All-Reduce} communication of these partial multiplications.
The third task pertains to updating the factor matrices $\Wm$ and $\Hm$ using matrices $\Wmt$ and $\Hmgr$, or $\Hmt$ and $\Wmgr$, which takes place at \cref{line:wmnnls,line:hmnnls}.
This corresponds to \cref{line:aunmf-w,line:aunmf-h} of \cref{alg:aunmf}, and can be computed locally at each process $p$ by executing \textsc{Nnls} algorithm on dense matrices $\Wmt(\Ip, :)$ and $\Hmgr$, or $\Hmt(:, \Jp)$ and $\Wmgr$, to obtain new $\Wm(\Ip, :)$ or $\Hm(\Ip, :)$, respectively, as described in \cref{sec:sparsenmfmu}.

The first type of communication in \cref{alg:distnmf} pertains to an \textsc{All-Reduce} of a dense matrix of fixed size $k \times k$ at \cref{line:hmgr,line:wmgr}, and the cost of this step is typically negligible in compare to the rest.
The other two communication types involve (i) transferring the partial row results of $\Wmt$ and $\Hmt$ to their owner processes at \cref{line:foldw,line:foldh} to accumulate at the owners, (ii) sending the updated rows of $\Wm$ and $\Hm$ to processes in need at \cref{line:expw,line:exph}.
We respectively call these steps \emph{fold} and \emph{expand} communications, following the convention used by the sparse matrix community.
The way these two communications are carried out plays a vital role in obtaining parallel scalability as they dominate the communication cost of the algorithm.


%Here, we employ an efficient point-to-point communication framework to effectuate fold and expand communications.
%In this scheme, after determining a partition of the matrix, each process $p$ determines the sets $\Fp$ and $\Gp$ using the row and column indices in its local matrix $\Amp$ and its sets $\Ip$ and $\Jp$ of owned row indices, then identifies the owners of these rows in order to request these rows from the corresponding processes in the expand step.
%Next, these requests are exchanged in a communication step so that each process $p$ determines the unique row indices that it needs to send to or receive from each other process in expand and fold steps.
%Once this communication structure is formed, it is reused througout all iterations in \cref{alg:distnmf}.
%This is possible thanks to the data partition staying fixed during the algorithm, hence data dependencies do not change.

%For convenience of the reader, \cref{tab:costs} discusses the computation, communication and memory complexity of this partitioning scheme that is observed in the \mpifaun algorithm. 
%To illustrate, in \cref{fig:partition} we provide a sparse matrix partitioned using a $5 \times 5$ checkerboard scheme where each process owns a subblock of the matrix, and show only the nonzero elements belonging to three columns with indices $j_1$, $j_2$, and $j_3$.
%Here, in performing the expand communication of \cref{line:exph}, the all-to-all strategy sends these column vectors to all processes in the same column, whereas point-to-point communication identifies the exact set of proceses in need beforehand, indicated with the same color, and communicates the data only with these processes.
%In this example, all-to-all scheme incurs 4, 3, and 2 units of redundant communication for columns $j_3$, $j_2$, and $j_1$, respectively, and this redundancy only worsens with the increasing number of processes.

\subsection{Communication Scheme}
\paragraph{Collective communication~(\textbf{COL}):} 
\mpifaun employs collective communication strategies for both expand and fold steps of \cref{alg:distnmf} for dense as well as sparse $\Am$, and partitions $\Am$ using a uniform checkerboard topology.
In this strategy, the rows and the columns indices $1, \dots, m$ and $1, \dots, n$ are divided into $P_r$ and $P_c$~($P = P_r P_c$) sets $I_1, \dots, I_{P_r}$ and $J_1, \dots, J_{P_c}$ of equal size and having contiguous indices.
Here, process $p$ owns the matrix subblock $\Am_{(r, c)}$, where $r = \lfloor P/P_c \rfloor + 1$ and $c = (P \mod P_c) + 1$, as well as $m / P$ and $n / P$ rows of $\Wm(I_r, :)$ and $\Hm(J_c, :)$.  
As $\Am_{(r, c)}$ only touches the rows/columns of processes in the same row/column of the processor grid, the communication of $\WW$ and $\HH$ are performed within each process row and column using \textsc{All-Gather} and \textsc{Reduce-Scatter} routines, in which the process $p$ receives all matrix rows $\Wm(I_r, :)$ and $\Hm(J_c, :)$ belonging to processes in the same row and column of the process grid.
Despite being favorable due to small number of exchanged messages in collective routines, in this strategy processes might receive rows that they do not need in their local sparse matrix dense matrix multiplication~(particularly if $\AA$ is very sparse), and this redundancy dramatically increases the communication volume, thus preventing scalability. 

\paragraph{Point-to-point communication~(\textbf{P2P}):} 
\hypertensor employs point-to-point communication for fold and expand steps by precomputing set of processes having a row/column in its footprint for each row/column of $\WW$/$\HH$.
This reduces the communication volume at the cost of increased number of messages with respect to the strategy of \mpifaun.


\begin{figure}
\caption{A 5x5 checkerboard partition of a sparse matrix.}
\label{fig:partition}
\includegraphics[width=0.8\linewidth]{figures/matrix-partition.png}
\end{figure}

\subsection{Partitioning} \label{sec:partitioning}

%This effectively defines a 2D uniform checkerboard partition of $\Am$ with matrix blocks $\Am_{(r,c)} = \Am(I_r, J_c)$ for all $1 \le r \le P_r$ and $1 \le c \le P_c$, as well as a row partition of $\Wm$ and $\Hm$.
%Here, process $p$ owns the matrix subblock $\Am_{(r, c)}$, where $r = \lfloor P/P_c \rfloor + 1$ and $c = (P \mod P_c) + 1$, as well as $m / P$ and $n / P$ rows of $\Wm(I_r, :)$ and $\Hm(J_c, :)$.
%

\Cref{alg:distnmf} requires a partitioning of the nonzeros of $\Am$ as well as the rows and the columns of $\Wm$ and $\Hm$, and these three partitions completely determine its computational and communication costs.
Here, we compare different partitioning strategies, employed by \mpifaun, \hypertensor, and SpMV kernels, and argue how they relate to these two performance metrics.

%Our algorithm is fine-grained; meaning that it can work any fine-grain, 1D, or 2D/checkerboard partition of the sparse matrix $\Am$.
%Here, however, we exclusively focus on $P_r \times P_c$~(where $P = P_r P_c$) checkerboard partitionings, and always set $P_c$ to the number of cores available per compute node in the cluster.
%This helps us reduce the communication cost in two ways.
%The first one corresponds to that we do not employ shared-memory parallelism in our implementation, hence assign one MPI rank per core.
%In this scenario, by assigning each group of $P_c$ processes owning the same row block of $\Am$ to the same node, and similarly assigning the corresponding row blocks of $\Wm$ and $\Wmt$ to these processes, we effectively eliminate the communication in the row dimension of the process grid~(which pertains to communicating matrices $\Wm$ and $\Wmt$) as these row exchanges stay within a node.
%This enables us to focus solely on reducing the communication due to columns of $\Hm$ and $\Hmt$.
%Second, this topology bounds the maximum number of messages sent per process by $P_r-1$ and $P_c-1$, thereby significantly reduce the communication latency by a factor of $P_c$ in compare with a fine-grain or a 1D partition, which incurs up to $P-1$ messages per process.

\subsubsection{Partitioning $\Am$} \label{sec:cp1d}

\paragraph{Checkerboard hypergraph partitioning~(\textbf{CH2})}
A hypergraph consists of vertices with associated weights and hyperedges that connect two or more vertices.
In the literature, a hypergraph is typically formed by adding a vertex for each computational task with the associated execution cost, adding a hyperedge for each data element, and connecting the vertex to a hyperedge whenever the associated task and the data are dependent.
Then, the vertices of the hypergraph is partitioned using a hypergraph partitioner to distribute vertex loads to parts equitably while reducing a metric called \emph{cutsize}, which amounts to minimizing the total number of different parts each hyperedge connects.
This corresponds in the actual computation to minimizing the data dependencies between tasks, hence the communication volume.

Traditional checkerboard hypergraph partitioning aims to partition the matrix $\Am$ into $P_r$ row slices first, and $P_c$ column slices next to obtain an $P_r \times P_c$ checkerboard partition~\cite{caay:99,aycu:08}.
The first partitioning phase is done using a column-net hypergraph model, in which for each row $\Am(i, :)$, a vertex $v_i$ with weight equaling to the number of nonzeros in $\Am(i, :)$ is created.
Each column $j$ is represented with a hyperedge $h_j$ and for each nonzero $(i, j) \in \Am$, which implies a dependency to $\Hm(:, j)$ in computing $\Wmt(i, :)$ at \cref{line:ah}, we connect $v_i$ to $h_j$.
This hypergraph is partitioned into $P_r$ parts giving the row partition of the checkerboard topology.
The second partitioning phase uses a row-net hypergraph model induced by this row partition, where each column is represented with a vertex with $P_r$ weights corresponding to the number of nonzeros in that column in all $P_r$ row segments.
Partitioning this hypergraph into $P_c$ parts finalizes the $P_r \times P_c$ checkerboard partitionin by balancing the weights~(number of nonzeros of $\Am_{(r, c)}$) of each part while minimizing the communication volume.
In the context of NMF, one issue arises when the matrix has some variance in the number of nonzeros in its rows/columns, which in turns yields unbalanced row/column strides.
This in turn creates an imbalance in the NNLS computations as rows/cols of $\WW$/$\HH$ are partitioned to the processes in the same stride.
To alleviate this issue, we modify this scheme slightly as follows.
In both row and column partitioning phases, we add an additional constant weight to vertices.
Balancing this additional constraint in hypergraph partitioning is expected to prevent such imbalanced strides.
This partitioning model~(which we call \textbf{CH2}) succesfully grasps the computation~(both SpMM and NNLS) and communication requirements using checkerboard topology for sparse NMF, yet is costly to compute in practice due to high number of constraints~($P_r + 1$) in the row-net hypergraph.

%We assign the value $(nnz(\Am) / m + nnz(\Am(i, :))$ to each vertex $v_i$.
%This weighting is preferred to find a tradeoff between balancing the number of vertices, which corresponds to the cost of dense matrix operations, and balancing the number of nonzeros per part, which corresponds to the cost of sparse matrix-dense matrix multiplication.
%We establish these two load balance goals by assigning a fixed cost to each vertex in the first summand, and increasing the cost of the vertices corresponding to denser rows in the second summand.
%We then partition this hypergraph into $R$ parts by balancing the vertex weights, and minimizing the cutsize of the hypergraph.
%In \cref{fig:partition} we also show this hypergraph construction for a $3 \times 3$ subset of the $A$ where rows corresponding to each vertex are highlighted and connected to related hyperedges.
%In this partitioning, minimizing the cutsize of this hypergraph exactly corresponds to minimizing the total communication volume due to columns of $\Hm$ and $\Hmt$ as is clear in \cref{fig:partition}, whereas balancing the vertex weights corresponds to balancing the cost of sparse matrix-dense matrix multiplications as well as dense matrix operations in computing Gram matrices and $\textsc{Nnls}$ in each process row.

%Once the matrix is partitioned into $R$ row slices, one can similarly partition columns of $\Am$ into $C$ parts using a multi-constraint formulation~\cite{aycu:08} using a \emph{row-net} hypergraph.
%Yet in our case, we skip this common approach for two reasons.
%First, existing multi-constraint partitioning tools are known to be not as efficient nor effective as single-constraint partitioners in practice.
%Second, the main goal of hypergraph partitioning of columns is to minimize the communication volume due to rows, whereas in our case communication of rows of $\Wm$ and $\Wmt$ stays within a node with a proper mapping of processes, and this communication cost would be negligible.
%Therefore, we instead partition columns randomly, which is expected to provide balance the number of nonzeros of $\Am$, number of columns of $\Hm$ and $\Hmt$, the amount of column communication volume due to columns of $\Hm$ and $\Hmt$.

\paragraph{1D-like checkerboard hypergraph partitioning~(\textbf{CH1})}
This variant partitions rows same as $\textbf{CH2}$, then partitions columns randomly to avoid multi-constraint partitioning.
Random column partition provides load and communication balance yet increases the communication volume for the rows of $\WW$.

\paragraph{Randomized checkerboard partitioning(\textbf{CRD})}
This scheme corresponds to partitioning both the rows and the columns of $\Am$ into $P_r$ and $P_c$ segments randomly.
It is expected to provide good load and communication balance both in sparse and dense matrix operations, but it overlooks the communication volume.

\paragraph{Uniform checkerboard partitioning(\textbf{CN})}
This partitioning variant forms an $P_R \times P_C$ partition of $\Am$ by putting a contiguous set of $m / P_R$ and $n / P_C$ rows and columns in each slice.
$\Wm$ and $\Hm$ are partitioned conformally with this topology; each process is assigned a contiguous set of  $m / P_R P_C$ and $n / P_R P_C$ rows and columns of $\Wm$ and $\Hm$.
This is the partitioning scheme employed by $\mpifaun$~\cite{KBP16, KBP16MPIFAUN}.
It provides perfect balance in NNLS step yet may incur high communication cost.
We also use a randomized variant (\urp) of this scheme in which the rows and columns of $\Am$ are permuted randomly to balance its nonzeros among parts.

\paragraph{Fine-grain hypergraph partitioning~(\textbf{FH})}
This is the partitioning strategy employed by \hypertensor.
It forms a fine-grain hypergraph involving a vertex for each nonzero $(\Am(i, j)$ and a hyperedge for each row and column index $i = 1, \dots, m$ and $j = 1, \dots, n$.
The resulting hypergraph is typically very large and is costly to partition, and unlike checkerboard variants fingerprint of processes are not restricted to a row/column stride.


\subsubsection{Partitioning $\Wm$ and $\Hm$}
Once $\Am$ is partitioned, one has to partition rows and columns of factor matrices to form the sets $\Ip$ and $\Jp$ in \cref{alg:distnmf}.
In doing so, we are interested in assigning rows and columns to processes equitably.
For this purpose, we specify imbalance parameters $\alpha$ that correspond to maximum imbalance we allow in this partitioning; i.e., $|\Ip| \le \alpha m / P$ and $|\Jp| \le \alpha n / P$ for each process $p$, and set $\alpha = 1.05$ in the experiments.

Next, for each row and column of $\Wm$ and $\Hm$ we create a list of processes that has a dependency to that row or column, which corresponds to processes owning the matrix blocks of same color in \cref{fig:partition}.
Finally, we randomly assign each row and column to one of the processes satisfying the imbalance constraint in this list.
If all processes in the list are overloaded, we assign it to the process that has the minimum number of rows/columns assigned.
For a checkerboard partition, the minimum is always chosen from the same processor row/column so that 2D communication topology is not disturbed.
Note that such an assignment increases the communication volume due to that row or column by 1; hence, in general smaller imbalance parameters yield larger communication volume due to increasing this type of assignment.
On the other hand, we desire to keep $\alpha$ small as it pertains to the load imbalance in the NNLS step.

%%\newcommand{\smaller}{\scriptsize}
%\begin{table*}[t!]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|}
%\hline
%\textbf{Algorithm} & \textbf{Flops} & \textbf{Words} & \textbf{Messages} & \textbf{Memory}  \\ \hline
%\smaller \mpifaun ($m/p < n$) & \smaller $O(2*nnz_p*k)+\frac{(m+n)k^2}{p}+F\lt(\frac mp,\frac np,k\rt)$ & \smaller $O\lt( \sqrt{\frac{mnk^2}{p}}\rt)$ & \smaller $O(\log p)^*$ & \smaller $O\lt(\frac{mn}{p}+\sqrt{\frac{mnk^2}{p}}\rt)$ \\ \hline
%\end{tabular}
%\normalsize
%\end{center}
%\caption{Leading order algorithmic costs for \NaiveAlg{} and \ParNMF{} (per iteration).  
%The function $F(\cdot)$ denotes the number of flops required for the particular NMF algorithm's Local Update Computation, aside from the matrix multiplications common across AU-NMF algorithms. \\
%$^*$The stated latency cost assumes no communication is required in \LUC; \HALS\ requires $k\log p$ messages for normalization steps.}
%\label{tab:costs}
%\end{table*}%
