%!TEX root = icpp18.tex


\section{Introduction}

%\begin{itemize}
%\item Introduce NMF
%\item Sparse NMF
%\item Sparse-dense matrix multiplication as bottleneck
%\item Hypergraph partition
%\item A partition that is not only good for multiplication but at the same time cannot hurt NNLS computation
%\item Introduce Contributions
%\end{itemize}

Non-negative Matrix Factorization (NMF) is the problem of finding two low rank factors $\WW\in \Rnplus{m\times k}$ and $\HH\in \Rnplus{k \times n}$ for a given input matrix  $\AA\in \Rnplus{m\times n}$, such that $\AA \approx \WW \HH$.
Here, $\Rnplus{m\times n}$ denotes the set of $m \times n$ matrices with non-negative real values.
Formally, the NMF problem can be defined as 
\SplitN{\label{eqn:origNMF}}{
\min_{\WW \geq 0,\HH \geq 0} & \|\AA-\WW\HH\|_F,
}
where $\|\M{X}\|_F=(\sum_{ij} x_{ij}^2)^{1/2}$ is the Frobenius norm. 
%The functions $\phi (\WW)$ and $\psi (\HH)$ are called regularization functions that avoid the model from over-fitting the data and are chosen depending on the characteristics of the data. 
%In this work, because we assume $\AA$ to be sparse, we use $\phi(\WW) = \alpha \|\WW\|_F^2$ (an $\ell_2$ regularizer) and $\psi(\HH) = \beta \sum_{i=1}^n \| \hh_{i} \|_1^2$ (an $\ell_1$ regularizer) to address the sparsity in the input matrix. 
%The lack of $\ell_1$ regularization on the matrix $\HH$ can result in numerical instability. 

NMF is widely used in data mining and machine learning as a dimension reduction and factor analysis method.
It is a natural fit for many real world problems as the non-negativity is inherent in many representations of real-world data and
the resulting low rank factors are expected to have a natural interpretation. The applications of NMF range from text mining \cite{pauca2004text},  computer vision \cite{hoyer2004non}, and bioinformatics \cite{kim2007sparse} to blind source separation  \cite{cichocki2009nonnegative}, unsupervised clustering \cite{kuang2012symmetric,kuang2013symnmf}  and many other areas.
In most real-world applications $m$ and $n$ are on the order of millions or more while $k$ is much smaller, on the order of tens to thousands.
Furthermore, data sets from these applications are often quite sparse and have highly irregular nonzero patterns.

%In the recent years, there has been an explosion in the size of collected data particularly with the widespread use of internet and social media.
%Such collected data has big dimensions~(from millions to billions) and is extremely sparse, which is either implied by the nature of the data~(e.g., most web content are not visited by users), or because capturing every possible data point is impractical.
%Analyzing such sparse big datasets require enormous computational power as well as efficient algorithms that can leverage the sparsity of the data.

%Thereby, we aim to enable analyzing data in such big data applications.

% How NMF is computed, and in sparse what is important.
The most common method for solving \cref{eqn:origNMF} is to use an alternating optimization approach, iteratively updating $\WW$ with $\HH$ fixed and then updating $\HH$ with $\WW$ fixed.
Specifically, updating a factor matrix involves three main operations; computing of $\WW^T\AA$ or $\AA\Hm^T$, computing the Gram matrix $\Wm^T \Wm$ or $\Hm \Hm^T$, and solving a non-negative linear least squares (NLS) problem using these two resulting matrices to update the factor matrix.
When the rank $k$ is small, the typical bottleneck is the computation that involves the input matrix, $\WW^T\AA$ and $\AA\Hm^T$, which are sparse-dense matrix multiplications (SpMMs) when the input data is sparse.
However, for large $k$, the Gram and NLS computations can become the bottleneck, as they grow more quickly with $k$ than the SpMMs.
Efficient parallel algorithms for NMF must load balance the computation and avoid communication overheads.
The distribution of the input $\AA$ across processors affects the load balance and communication of the SpMMs, and the distribution of the factor matrices $\WW$ and $\HH$ affects the load balance of the Gram and NLS computations.

There is a lack of highly scalable parallel algorithms and software for computing sparse NMF.
MPI-FAUN \cite{KBP17} software framework enables computing the NMF for dense and sparse data, yet the algorithm is optimized for dense input matrices and employs the same communication scheme to carry out sparse NMF computations.
It uses a regular, Cartesian partitioning of the input matrix $\AA$ across processors that is oblivious to the nonzero pattern.
The advantages of this approach are that the resulting factor matrix communication (to compute the SpMMs) is also regular and cast as low-latency MPI collective operations and that the rest of the computation (including NLS updates) is load-balanced.
The disadvantage is that because the partition ignores the sparsity of $\AA$, the approach will often communicate more data than necessary, as some processors will receive data that they do not need for local computation.
For sparse tensor factorization, there are scalable algorithms and software resembling NMF kernels with sparse irregular computational patterns~\cite{KU15}, yet they require an expensive partitioning step and do not necessarily yield the optimal performance for NMF.
The main advantage of this approach is that it minimizes the communication cost of the SpMM step through hypergraph partitioning, and that it employs point-to-point communication scheme which communicates elements of $\WW$ and $\HH$ only to processors in need.
The disadvantages of this approach are an upfront hypergraph partitioning cost, possible load imbalance~(particularly in the NLS computations) and communication imbalance.
Our goal is in this paper is to fill in this gap between two approaches by comparing and evaluating them in the context of NMF, then proposing a synthesis involving parallel algorithms as well as communication and partitioning schemes enabling scalability to thousands of processes.

%To the best of our knowledge, the only high performance software available for parallelizing this computation is \mpifaun~\cite{KBP16MPIFAUN} which operates both on sparse and dense input matrices.
%For parallelism, it employs a fixed 2D uniform partitioning on $\Am$, and partitions factor matrices $\Wm$ and $\Hm$ conformally with this 2D partition.
%Each process, works on its local matrix block of $\Am$, and communicates the rows and columns of $\Wm$ and $\Hm$ corresponding to its process row and column in the 2D topology.
%This communication is optimal in the dense case; i.e., no process receives a data element not used in its local computations.
%In the sparse case, however, the actual communication requirements of processes depend on the sparsity of $\Am$ and its partitioning, and is in general significantly less than the dense case.
%Therefore, employing the same all-to-all communication scheme in the sparse case brings about a major redundancy in the communication cost.
%Also, \mpifaun similarly uses only fixed 2D uniform partitioning on sparse matrices; this creates significant imbalance both in computation and communication as typically the data is not homogenously distributed in $\Am$.
%In overall, the lack of an efficient sparse communication scheme that allows flexibility in partitioning hinders the scalability of this approach.

% Sketch of our software and advantages over MPIFAUN
%In this paper, we provide an efficient parallel NMF algorithm called \distspnmf for sparse matrices, which leverages the sparsity of the matrix, and employs an optimal point-to-point communication scheme that prevents any redundant communication.
%Moreover, our algorithm is flexible in the sense that it can work on any partition of $\Am$ as well as factor matrices $\Wm$ and $\Hm$.
%We use this partitioning potential in two ways.
%First, we employ a 2D cartesion processor topology to eliminate the communication cost in one dimension, and bound the maximum number of messages sent per process.
%Then, we investigate efficient 2D partitioning strategies to embed into this topology iin order to further reduce the communication cost and enhance scalability.


%% List contributions
%
%\todo{\oguz{revise this paragraph}}
%I
%In this paper, we are leveraging the \mpifaun algorithm \cite{KBP16MPIFAUN} which established the high performance framework for different NMF algorithms. NMF is a dimensionality reduction problem and there are various algorithms exists in the literature. The major steps in NMF algorithms are (a)matrix multiplication between the sparse input matrix $\AA$ with the low rank factors $\WW$ or $\HH$  (b) solving Non-negative Least Squares (NLS) subproblems and (c) computing the gram matrix. For the convenience of the readers, we are presenting the general NMF Algorithm \ref{alg:aunmf} based on \mpifaun. The different NMF algorithms such as Multiplicative Update \cite{seung2001algorithms},Hierarchical Alternating Least Square \cite{cichocki2009nonnegative} and Alternative Non-negative Least Squares using Block Principal Pivoting \cite{kim2013nonnegative} differ in the way they use the matrix multiplication and gram of the factor matrix to obtain the NLS factor. 
%
%For sparse NMF, the two popular approaches for scalability are distributing non-zeros \todo{\oguz{add a citation}} ($NNZ-PART$) and the sparse matrix itself\cite{KBP16MPIFAUN, KBP16} ($2D$).  For the latter, 2D distributions are effective for reducing the communication cost and the \mpifaun algorithm  \cite{KBP16MPIFAUN} claims communication minimization for dense matrices under some mild assumptions.  %Even though this can work for some sparse input matrix as well, there are some shortcomings in the existing framework and in Section \ref{sec:sparsenmf}, we explain our key extensions handle really large high dimensional sparse data.  
%Even though this can work for some sparse input matrix as well, there is a huge load imbalance which is typically alleviated by permuting the rows and columns of the matrix. 
%Also, the performance of $NNZ-PART$ can be more useful in practice over $2D$ partition. However, it is very difficult to design an effective non-zero distribution strategy for any general input sparse matrix that balances the load and reduces the communication. 
%
%We will discuss briefly these two issues of computation and communication in the case of sparse NMF. The $NNZ-PART$ of sparse NMF should take into consideration all the three operations - sparse-dense matrix multiplication, NLS and gram of the factor matrices. On very large number of processors when the problem size has become smaller, gram is as important as other operations. The $NNZ-PART$ and $2D$ employs completely different communication MPI techniques. While $2D$ partitions leverage the MPI collective communication which is typically bounded logarithm of the number of processors $p$, the $NNZ-PART$ uses point-to-point($P2P$) communication which is bounded by the number and the size of messages. The major disadvantage of $P2P$ communication is the latency which most of the time in practice dominates the communication cost. 
%
%Towards this end, we are proposing a novel Algorithm \distspnmffull(\distspnmf) that handles both the distribution strategies $NNZ-PART$ and $2D$. We are proposing two $NNZ-PART$ schemes (i) Checkerboard-PaTOH (\cpp)  and (ii) Checkerboard Random (\crp). These schemes consider multiple constraints such as (a) effective for the three operations in NMF algorithm (b) reducing the number of messages between nodes  as the communication within a computational node is insignificant and (c) \todo{\oguz{add additional constraints}}. It is important to notice that our algorithm is independent of the partition schemes employed for $NNZ-PART$ and it is not restricted to only the proposed $\cpp$ and $\crp$.

%\grey{I have not edited the following contribution list yet, we may want to wait until we have new data...
%We summarize our main contributions as follows:
%\begin{itemize}
%\item We propose an efficient parallel NMF algorithm called \distspnmf for sparse matrices that employs a point-to-point communication scheme to leverage the sparsity of the input matrix, and eliminate any redundant communication existing in \mpifaun.
%The algorithm is flexible to work with any partition of the input matrix $\Am$ as well as factor matrices $\Wm$ and $\Hm$.
%\item We employ a 2D cartesion process topology to eliminate the communication cost of parallel NMF in one dimension, and bound the maximum number of messages sent per process in our MPI implementation.
%\item We introduce effective partitioning strategies to further reduce the communication cost and enhance the parallel scalability of our algorithm.
%\item We introduce regularization to NMF computations to prevent potential numerical instabilities using large matrices.
%\item We compare our implementation with a state-of-the-art NMF library, and report scalability results up to 32678 processors on two different parallel computing platforms using real-world datasets.
%\end{itemize}
%}
