\begin{abstract}
Non-negative matrix factorization (NMF) is the problem of finding two non-negative low rank factors $\WW$ and $\HH$ for a given input matrix $\AA$ such that $\AA \approx \WW \HH$.
NMF is a useful tool for many internet applications such as topic modeling in text mining and community detection in social networks.
In this paper, we focus on scaling distributed NMF to very large sparse datasets by employing effective algorithms, communications, and partitioning schemes that leverage the sparsity of the input matrix $\AA$.
Our method employs an optimal point-to-point communication scheme, and removes any redundant communication existing in the state of the art.
Unlike the state of the art, our method also permits arbitrary partitions of the sparse input matrix as well as factor matrices, which enables us to employ effective partitioning schemes for better scalability.
We experiment on a cluster using up to 3072 cores, and report significant better scalability with respect to existing methods owing to our efficient communication and partitioning strategies.
We also run our algorithms on an IBM BlueGene/Q supercomputer, and achieve scalability up to 32768 processors with a 32x speedup over the execution using 256 processors.

%We propose a fine-grain communication scheme that can utilize arbitrary partitions of the matrices. For a more scalable parallel NMF algorithm, we employ effective 2D and fine-grain hypergraph partitions. If hypergraph partitioning gets expensive for the purpose of the application, researchers can resort to the proposed uniform and random 2D partitioning NMF algorithms. But in the latter, memory growth of intermediate dense matrices is exorbitant -- a major bottleneck to run in lesser number of nodes. Toward this, we propose memory efficient blocking 2D matrix multiplication for sparse NMF. We compare the performance of 2D and the hypergraph partitions on real-world datasets. 
\end{abstract}
